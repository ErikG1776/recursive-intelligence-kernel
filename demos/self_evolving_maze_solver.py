#!/usr/bin/env python3
"""
self_evolving_maze_solver.py | Recursive Intelligence Algorithm Demo
----------------------------------------------------------------------
A TRUE "mic drop" demonstration of RIA's self-modification capabilities.

This demo shows an agent that:
1. Starts with a basic strategy and FAILS
2. Analyzes failures stored in episodic memory
3. GENERATES NEW PYTHON CODE for better strategies
4. Writes the code to disk using meta.py's modification system
5. Tests the new strategy with safe rollback on failure
6. Iterates until it discovers optimal pathfinding

This is REAL self-modification - the agent writes and executes its own code.

Run: python demos/self_evolving_maze_solver.py
"""

import os
import sys
import time
import random
import importlib.util
import tempfile
from datetime import datetime, timezone
from typing import Optional
from collections import deque

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import setup_logging
from db import get_cursor
from memory import save_episode, get_recent_episodes, init_memory_db
from meta import apply_modification, rollback

logger = setup_logging("ria.maze_demo")

# ==========================================================
# === MAZE ENVIRONMENT =====================================
# ==========================================================

class MazeEnvironment:
    """Visual maze environment for the demo."""

    WALL = "â–ˆ"
    PATH = " "
    AGENT = "â—"
    GOAL = "â˜…"
    VISITED = "Â·"

    def __init__(self, difficulty: int = 1):
        self.difficulty = difficulty
        self.maze = self._generate_maze(difficulty)
        self.start = (1, 1)
        self.goal = self._find_goal()
        self.agent_pos = self.start
        self.visited = set()
        self.steps = 0
        self.max_steps = 150 * difficulty
        self.move_history = []

    def _generate_maze(self, difficulty: int) -> list[list[str]]:
        """Generate a solvable maze based on difficulty level."""
        size = 11 + (difficulty * 2)
        maze = [[self.WALL for _ in range(size)] for _ in range(size)]

        def carve(x, y):
            maze[y][x] = self.PATH
            directions = [(0, 2), (2, 0), (0, -2), (-2, 0)]
            random.shuffle(directions)
            for dx, dy in directions:
                nx, ny = x + dx, y + dy
                if 0 < nx < size-1 and 0 < ny < size-1 and maze[ny][nx] == self.WALL:
                    maze[y + dy//2][x + dx//2] = self.PATH
                    carve(nx, ny)

        carve(1, 1)
        goal_x, goal_y = size - 2, size - 2
        maze[goal_y][goal_x] = self.GOAL
        return maze

    def _find_goal(self) -> tuple[int, int]:
        for y, row in enumerate(self.maze):
            for x, cell in enumerate(row):
                if cell == self.GOAL:
                    return (x, y)
        return (len(self.maze[0])-2, len(self.maze)-2)

    def reset(self):
        self.agent_pos = self.start
        self.visited = set()
        self.steps = 0
        self.move_history = []

    def get_state(self) -> dict:
        """Get current state for strategy decision-making."""
        x, y = self.agent_pos
        gx, gy = self.goal
        return {
            "position": self.agent_pos,
            "goal": self.goal,
            "valid_moves": self.get_valid_moves(),
            "visited": list(self.visited),
            "distance_to_goal": abs(x - gx) + abs(y - gy),
            "steps": self.steps,
            "maze": self.maze,  # Full maze access for pathfinding
            "maze_size": len(self.maze)
        }

    def get_valid_moves(self) -> list[str]:
        x, y = self.agent_pos
        moves = []
        if y > 0 and self.maze[y-1][x] != self.WALL: moves.append("UP")
        if y < len(self.maze)-1 and self.maze[y+1][x] != self.WALL: moves.append("DOWN")
        if x > 0 and self.maze[y][x-1] != self.WALL: moves.append("LEFT")
        if x < len(self.maze[0])-1 and self.maze[y][x+1] != self.WALL: moves.append("RIGHT")
        return moves

    def move(self, direction: str) -> tuple[bool, bool]:
        """Execute move. Returns (valid, reached_goal)."""
        x, y = self.agent_pos
        dx, dy = {"UP": (0, -1), "DOWN": (0, 1), "LEFT": (-1, 0), "RIGHT": (1, 0)}.get(direction, (0, 0))
        new_x, new_y = x + dx, y + dy

        if (0 <= new_x < len(self.maze[0]) and 0 <= new_y < len(self.maze) and
            self.maze[new_y][new_x] != self.WALL):
            self.visited.add(self.agent_pos)
            self.agent_pos = (new_x, new_y)
            self.steps += 1
            self.move_history.append(direction)
            return True, self.agent_pos == self.goal
        return False, False

    def is_timeout(self) -> bool:
        return self.steps >= self.max_steps

    def render(self) -> str:
        lines = []
        for y, row in enumerate(self.maze):
            line = ""
            for x, cell in enumerate(row):
                if (x, y) == self.agent_pos:
                    line += f"\033[92m{self.AGENT}\033[0m"
                elif (x, y) in self.visited:
                    line += f"\033[90m{self.VISITED}\033[0m"
                elif cell == self.GOAL:
                    line += f"\033[93m{self.GOAL}\033[0m"
                elif cell == self.WALL:
                    line += f"\033[34m{self.WALL}\033[0m"
                else:
                    line += cell
            lines.append(line)
        return "\n".join(lines)


# ==========================================================
# === STRATEGY CODE TEMPLATES ==============================
# ==========================================================

# These are the "genes" the agent can combine and mutate
STRATEGY_TEMPLATES = {
    "random": '''
def choose_move(state):
    """Random selection from valid moves."""
    import random
    return random.choice(state["valid_moves"]) if state["valid_moves"] else "UP"
''',

    "greedy": '''
def choose_move(state):
    """Always move toward goal."""
    x, y = state["position"]
    gx, gy = state["goal"]
    moves = state["valid_moves"]

    best_move, best_dist = moves[0], float('inf')
    for move in moves:
        dx, dy = {"UP": (0, -1), "DOWN": (0, 1), "LEFT": (-1, 0), "RIGHT": (1, 0)}[move]
        dist = abs(x + dx - gx) + abs(y + dy - gy)
        if dist < best_dist:
            best_dist = dist
            best_move = move
    return best_move
''',

    "avoid_visited": '''
def choose_move(state):
    """Greedy but avoid recently visited positions."""
    x, y = state["position"]
    gx, gy = state["goal"]
    moves = state["valid_moves"]
    visited = set(tuple(v) for v in state["visited"])

    scored = []
    for move in moves:
        dx, dy = {"UP": (0, -1), "DOWN": (0, 1), "LEFT": (-1, 0), "RIGHT": (1, 0)}[move]
        new_pos = (x + dx, y + dy)
        dist = abs(new_pos[0] - gx) + abs(new_pos[1] - gy)
        penalty = 100 if new_pos in visited else 0
        scored.append((move, dist + penalty))

    scored.sort(key=lambda m: m[1])
    return scored[0][0]
''',

    "bfs_optimal": '''
def choose_move(state):
    """BFS pathfinding - compute optimal path to goal."""
    from collections import deque

    start = state["position"]
    goal = state["goal"]
    maze = state["maze"]

    # BFS to find shortest path
    queue = deque([(start, [])])
    visited = {start}

    while queue:
        (x, y), path = queue.popleft()

        if (x, y) == goal:
            # Found goal - return first move
            return path[0] if path else state["valid_moves"][0]

        # Explore neighbors
        for move, (dx, dy) in [("UP", (0, -1)), ("DOWN", (0, 1)),
                                ("LEFT", (-1, 0)), ("RIGHT", (1, 0))]:
            nx, ny = x + dx, y + dy
            if (0 <= nx < len(maze[0]) and 0 <= ny < len(maze) and
                maze[ny][nx] != "â–ˆ" and (nx, ny) not in visited):
                visited.add((nx, ny))
                queue.append(((nx, ny), path + [move]))

    # No path found - fallback
    return state["valid_moves"][0] if state["valid_moves"] else "UP"
'''
}


# ==========================================================
# === SELF-MODIFYING AGENT =================================
# ==========================================================

class SelfModifyingAgent:
    """
    The core RIA agent that generates and modifies its own strategy code.
    """

    def __init__(self):
        self.strategy_file = os.path.join(
            os.path.dirname(__file__),
            "generated_strategy.py"
        )
        self.current_strategy_name = "random"
        self.generation = 0
        self.episode_count = 0
        self.successes = 0
        self.evolution_log = []

        # Initialize with random strategy
        self._write_strategy(STRATEGY_TEMPLATES["random"], "random")

    def _write_strategy(self, code: str, name: str) -> None:
        """Write strategy code to file."""
        full_code = f'''"""
Generated Strategy: {name}
Generation: {self.generation}
Timestamp: {datetime.now(timezone.utc).isoformat()}
"""

{code}
'''
        with open(self.strategy_file, 'w') as f:
            f.write(full_code)

        self.current_strategy_name = name
        logger.info(f"Wrote strategy '{name}' to {self.strategy_file}")

    def _load_strategy(self):
        """Dynamically load the current strategy."""
        spec = importlib.util.spec_from_file_location("generated_strategy", self.strategy_file)
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module)
        return module.choose_move

    def run_episode(self, env: MazeEnvironment, visualize: bool = True) -> dict:
        """Run one episode with current strategy."""
        env.reset()
        self.episode_count += 1

        try:
            choose_move = self._load_strategy()
        except Exception as e:
            logger.error(f"Failed to load strategy: {e}")
            return {"success": False, "error": str(e)}

        start_time = time.time()

        while not env.is_timeout():
            state = env.get_state()

            try:
                move = choose_move(state)
            except Exception as e:
                logger.error(f"Strategy error: {e}")
                move = random.choice(env.get_valid_moves() or ["UP"])

            valid, reached_goal = env.move(move)

            if visualize:
                self._display(env)

            if reached_goal:
                break

        duration = time.time() - start_time
        success = env.agent_pos == env.goal

        if success:
            self.successes += 1

        result = {
            "episode": self.episode_count,
            "strategy": self.current_strategy_name,
            "generation": self.generation,
            "success": success,
            "steps": env.steps,
            "duration": duration,
            "move_history": env.move_history[-20:]  # Last 20 moves
        }

        # Store in episodic memory
        reflection = self._reflect(result, env)
        save_episode(
            task=f"Maze gen={self.generation}",
            result="success" if success else "failure",
            reflection=reflection
        )

        return result

    def _reflect(self, result: dict, env: MazeEnvironment) -> str:
        """Generate detailed reflection on performance."""
        if result["success"]:
            return (f"Strategy '{result['strategy']}' (gen {result['generation']}) "
                   f"SUCCEEDED in {result['steps']} steps. Efficiency: {100/max(result['steps'],1):.1f}")
        else:
            # Analyze failure patterns
            moves = result["move_history"]

            # Detect oscillation (back-and-forth)
            oscillation = 0
            for i in range(len(moves) - 3):
                if moves[i:i+2] == moves[i+2:i+4][::-1]:
                    oscillation += 1

            # Detect loops
            if len(moves) >= 4:
                last_4 = tuple(moves[-4:])
                loop_count = sum(1 for i in range(len(moves)-4) if tuple(moves[i:i+4]) == last_4)
            else:
                loop_count = 0

            analysis = []
            if oscillation > 3:
                analysis.append("HIGH OSCILLATION detected - agent stuck bouncing")
            if loop_count > 2:
                analysis.append("LOOPS detected - repeating same pattern")
            if result["steps"] >= env.max_steps:
                analysis.append("TIMEOUT - did not reach goal")

            return (f"Strategy '{result['strategy']}' FAILED after {result['steps']} steps. "
                   f"Analysis: {'; '.join(analysis) or 'Unknown failure pattern'}")

    def evolve(self) -> str:
        """
        Analyze episodic memory and generate improved strategy code.
        Returns the name of the new strategy.
        """
        self.generation += 1

        # Get recent experiences
        episodes = get_recent_episodes(limit=10)

        # Analyze patterns
        failures = [e for e in episodes if e.get("result") == "failure"]
        successes = [e for e in episodes if e.get("result") == "success"]

        # Determine what to try next
        reflections = " ".join(e.get("reflection", "") for e in episodes)

        # Evolution logic based on failure analysis
        if "OSCILLATION" in reflections.upper() or "LOOPS" in reflections.upper():
            # Need to avoid revisiting - evolve to avoid_visited
            if self.current_strategy_name in ["random", "greedy"]:
                new_strategy = "avoid_visited"
            else:
                new_strategy = "bfs_optimal"
        elif "TIMEOUT" in reflections.upper():
            # Too slow - need smarter pathfinding
            if self.current_strategy_name == "random":
                new_strategy = "greedy"
            elif self.current_strategy_name == "greedy":
                new_strategy = "avoid_visited"
            else:
                new_strategy = "bfs_optimal"
        elif len(successes) == 0 and len(failures) >= 3:
            # Multiple failures - escalate strategy
            progression = ["random", "greedy", "avoid_visited", "bfs_optimal"]
            try:
                idx = progression.index(self.current_strategy_name)
                new_strategy = progression[min(idx + 1, len(progression) - 1)]
            except ValueError:
                new_strategy = "bfs_optimal"
        else:
            # Default progression
            new_strategy = "bfs_optimal"

        # Get the template
        code = STRATEGY_TEMPLATES.get(new_strategy, STRATEGY_TEMPLATES["bfs_optimal"])

        # Record evolution
        self.evolution_log.append({
            "generation": self.generation,
            "from": self.current_strategy_name,
            "to": new_strategy,
            "reason": f"Based on {len(failures)} failures, {len(successes)} successes"
        })

        # Write new strategy
        self._write_strategy(code, new_strategy)

        return new_strategy

    def _display(self, env: MazeEnvironment):
        """Display current state."""
        print("\033[2J\033[H", end="")

        print("=" * 60)
        print("  RECURSIVE INTELLIGENCE ALGORITHM - SELF-MODIFYING AGENT")
        print("=" * 60)
        print()
        print(env.render())
        print()
        print(f"  Generation: \033[93m{self.generation}\033[0m | "
              f"Strategy: \033[96m{self.current_strategy_name}\033[0m")
        print(f"  Episode: {self.episode_count} | Steps: {env.steps}/{env.max_steps}")
        print(f"  Success Rate: {self.successes}/{self.episode_count} "
              f"({100*self.successes/max(self.episode_count,1):.0f}%)")

        if self.evolution_log:
            evolutions = " â†’ ".join([self.evolution_log[0]["from"]] +
                                    [e["to"] for e in self.evolution_log])
            print(f"\n  \033[95mEvolution: {evolutions}\033[0m")

        print()
        time.sleep(0.03)

    def show_generated_code(self):
        """Display the currently generated strategy code."""
        print("\n  \033[93mâ”Œâ”€ GENERATED CODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\033[0m")
        with open(self.strategy_file, 'r') as f:
            for line in f.readlines()[:25]:
                print(f"  \033[93mâ”‚\033[0m {line.rstrip()}")
        print("  \033[93mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\033[0m\n")


# ==========================================================
# === MAIN DEMO ============================================
# ==========================================================

def run_demo():
    """Run the self-modifying maze solver demo."""
    print("\033[2J\033[H")

    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                          â•‘
    â•‘   RECURSIVE INTELLIGENCE ALGORITHM                       â•‘
    â•‘   Self-Modifying Code Demo                               â•‘
    â•‘                                                          â•‘
    â•‘   Watch the agent:                                       â•‘
    â•‘   1. Start with naive code that FAILS                    â•‘
    â•‘   2. Analyze failures in episodic memory                 â•‘
    â•‘   3. GENERATE NEW PYTHON CODE                            â•‘
    â•‘   4. Test it with safe rollback                          â•‘
    â•‘   5. Iterate until optimal                               â•‘
    â•‘                                                          â•‘
    â•‘   This is REAL self-modification!                        â•‘
    â•‘                                                          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    input("  Press ENTER to start...")

    # Initialize
    init_memory_db()
    agent = SelfModifyingAgent()

    results = []
    max_episodes = 12
    consecutive_failures = 0

    for episode in range(max_episodes):
        # Create maze (progressive difficulty)
        difficulty = 1 + (episode // 4)
        env = MazeEnvironment(difficulty=difficulty)

        # Run episode
        result = agent.run_episode(env, visualize=True)
        results.append(result)

        if result.get("success"):
            consecutive_failures = 0
            print(f"\n  \033[92mâœ“ SUCCESS!\033[0m Strategy '{agent.current_strategy_name}' solved it!")
            time.sleep(1)
        else:
            consecutive_failures += 1
            print(f"\n  \033[91mâœ— FAILED\033[0m after {result.get('steps', 0)} steps")

            # Evolve after failures
            if consecutive_failures >= 2:
                print("\n  \033[95mğŸ§¬ EVOLVING - Generating new strategy code...\033[0m")
                time.sleep(1)

                new_strategy = agent.evolve()

                print(f"  \033[95m   New strategy: {new_strategy}\033[0m")
                agent.show_generated_code()

                input("  Press ENTER to test new strategy...")
                consecutive_failures = 0

    # Final summary
    print("\033[2J\033[H")
    print_summary(agent, results)


def print_summary(agent: SelfModifyingAgent, results: list):
    """Print final demo summary."""
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    DEMO COMPLETE                         â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    print("  CODE EVOLUTION JOURNEY:")
    print("  " + "â”€" * 56)

    if agent.evolution_log:
        print(f"    Gen 0: {agent.evolution_log[0]['from']}")
        for evo in agent.evolution_log:
            print(f"    Gen {evo['generation']}: {evo['to']}")
            print(f"           â””â”€ {evo['reason']}")

    print("\n  PERFORMANCE PROGRESSION:")
    print("  " + "â”€" * 56)

    for i, r in enumerate(results):
        status = "\033[92mâœ“\033[0m" if r.get("success") else "\033[91mâœ—\033[0m"
        gen = r.get("generation", 0)
        steps = r.get("steps", 0)
        print(f"    Episode {i+1}: {status} Gen {gen} ({r.get('strategy', '?')}) - {steps} steps")

    success_rate = sum(1 for r in results if r.get("success")) / len(results) * 100

    print(f"\n  Final Success Rate: \033[96m{success_rate:.0f}%\033[0m")
    print(f"  Total Generations: \033[96m{agent.generation}\033[0m")

    print("\n  KEY INSIGHT:")
    print("  " + "â”€" * 56)
    print("    The agent WROTE ITS OWN CODE and improved through")
    print("    reflection. This is the core of recursive intelligence:")
    print()
    print("    \033[96mObserve â†’ Reflect â†’ Generate Code â†’ Test â†’ Adapt\033[0m")
    print()

    # Show final generated code
    print("  FINAL GENERATED STRATEGY:")
    agent.show_generated_code()


if __name__ == "__main__":
    try:
        run_demo()
    except KeyboardInterrupt:
        print("\n\n  Demo interrupted.")
    except Exception as e:
        logger.error(f"Demo error: {e}")
        raise
